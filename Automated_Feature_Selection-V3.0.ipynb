{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for the war against the virus\n",
    "## Automated pipeline for feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayelet\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Ayelet\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_similarity_score\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"input/home_ex_raw_data.csv\",index_col='serial_number')\n",
    "raw_data.drop(raw_data.columns[[0]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_#_0</th>\n",
       "      <th>test_#_1</th>\n",
       "      <th>test_#_2</th>\n",
       "      <th>test_#_3</th>\n",
       "      <th>test_#_4</th>\n",
       "      <th>test_#_5</th>\n",
       "      <th>test_#_6</th>\n",
       "      <th>test_#_7</th>\n",
       "      <th>test_#_8</th>\n",
       "      <th>test_#_9</th>\n",
       "      <th>test_#_10</th>\n",
       "      <th>test_#_11</th>\n",
       "      <th>test_#_12</th>\n",
       "      <th>test_#_13</th>\n",
       "      <th>test_#_14</th>\n",
       "      <th>test_#_15</th>\n",
       "      <th>test_#_16</th>\n",
       "      <th>test_#_17</th>\n",
       "      <th>test_#_18</th>\n",
       "      <th>test_#_19</th>\n",
       "      <th>test_#_20</th>\n",
       "      <th>test_#_21</th>\n",
       "      <th>test_#_22</th>\n",
       "      <th>test_#_23</th>\n",
       "      <th>test_#_24</th>\n",
       "      <th>test_#_25</th>\n",
       "      <th>test_#_26</th>\n",
       "      <th>test_#_27</th>\n",
       "      <th>test_#_28</th>\n",
       "      <th>test_#_29</th>\n",
       "      <th>test_#_30</th>\n",
       "      <th>test_#_31</th>\n",
       "      <th>test_#_32</th>\n",
       "      <th>test_#_33</th>\n",
       "      <th>test_#_34</th>\n",
       "      <th>test_#_35</th>\n",
       "      <th>test_#_36</th>\n",
       "      <th>test_#_37</th>\n",
       "      <th>test_#_38</th>\n",
       "      <th>test_#_39</th>\n",
       "      <th>test_#_40</th>\n",
       "      <th>test_#_41</th>\n",
       "      <th>test_#_42</th>\n",
       "      <th>test_#_43</th>\n",
       "      <th>test_#_44</th>\n",
       "      <th>test_#_45</th>\n",
       "      <th>test_#_46</th>\n",
       "      <th>test_#_47</th>\n",
       "      <th>test_#_48</th>\n",
       "      <th>test_#_49</th>\n",
       "      <th>...</th>\n",
       "      <th>test_#_1514</th>\n",
       "      <th>test_#_1515</th>\n",
       "      <th>test_#_1516</th>\n",
       "      <th>test_#_1517</th>\n",
       "      <th>test_#_1518</th>\n",
       "      <th>test_#_1519</th>\n",
       "      <th>test_#_1520</th>\n",
       "      <th>test_#_1521</th>\n",
       "      <th>test_#_1522</th>\n",
       "      <th>test_#_1523</th>\n",
       "      <th>test_#_1524</th>\n",
       "      <th>test_#_1525</th>\n",
       "      <th>test_#_1526</th>\n",
       "      <th>test_#_1527</th>\n",
       "      <th>test_#_1528</th>\n",
       "      <th>test_#_1529</th>\n",
       "      <th>test_#_1530</th>\n",
       "      <th>test_#_1531</th>\n",
       "      <th>test_#_1532</th>\n",
       "      <th>test_#_1533</th>\n",
       "      <th>test_#_1534</th>\n",
       "      <th>test_#_1535</th>\n",
       "      <th>test_#_1536</th>\n",
       "      <th>test_#_1537</th>\n",
       "      <th>test_#_1538</th>\n",
       "      <th>test_#_1539</th>\n",
       "      <th>test_#_1540</th>\n",
       "      <th>test_#_1541</th>\n",
       "      <th>test_#_1542</th>\n",
       "      <th>test_#_1543</th>\n",
       "      <th>test_#_1544</th>\n",
       "      <th>test_#_1545</th>\n",
       "      <th>test_#_1546</th>\n",
       "      <th>test_#_1547</th>\n",
       "      <th>test_#_1548</th>\n",
       "      <th>test_#_1549</th>\n",
       "      <th>test_#_1550</th>\n",
       "      <th>test_#_1551</th>\n",
       "      <th>test_#_1552</th>\n",
       "      <th>test_#_1553</th>\n",
       "      <th>test_#_1554</th>\n",
       "      <th>test_#_1555</th>\n",
       "      <th>test_#_1556</th>\n",
       "      <th>test_#_1557</th>\n",
       "      <th>test_#_1558</th>\n",
       "      <th>test_#_1559</th>\n",
       "      <th>test_#_1560</th>\n",
       "      <th>test_#_1561</th>\n",
       "      <th>test_#_1562</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serial_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SZ1218-020512D1C-BA</th>\n",
       "      <td>24631336</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>542190876.0</td>\n",
       "      <td>1</td>\n",
       "      <td>518426.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>1</td>\n",
       "      <td>333091.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>333096.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>109.0</td>\n",
       "      <td>19.95112</td>\n",
       "      <td>1</td>\n",
       "      <td>333138.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>110.0</td>\n",
       "      <td>333098.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>333092.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>333097.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ1218-020512C9D-3A</th>\n",
       "      <td>24631416</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>542190749.0</td>\n",
       "      <td>1</td>\n",
       "      <td>518426.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>2</td>\n",
       "      <td>333091.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>333096.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>109.0</td>\n",
       "      <td>19.89527</td>\n",
       "      <td>1</td>\n",
       "      <td>333138.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>110.0</td>\n",
       "      <td>333098.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>333092.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>333097.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ1218-020512802-9B</th>\n",
       "      <td>24631650</td>\n",
       "      <td>295.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>542189570.0</td>\n",
       "      <td>1</td>\n",
       "      <td>518426.0</td>\n",
       "      <td>1</td>\n",
       "      <td>295.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>1</td>\n",
       "      <td>333091.0</td>\n",
       "      <td>1</td>\n",
       "      <td>295.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>333096.0</td>\n",
       "      <td>1</td>\n",
       "      <td>295.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>109.0</td>\n",
       "      <td>20.06003</td>\n",
       "      <td>1</td>\n",
       "      <td>333138.0</td>\n",
       "      <td>1</td>\n",
       "      <td>295.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>110.0</td>\n",
       "      <td>333098.0</td>\n",
       "      <td>1</td>\n",
       "      <td>295.0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>333092.0</td>\n",
       "      <td>1</td>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2.018011e+13</td>\n",
       "      <td>333097.0</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     test_#_0  test_#_1  test_#_2  test_#_3  test_#_4  \\\n",
       "serial_number                                                           \n",
       "SZ1218-020512D1C-BA  24631336     246.0         1     159.0         1   \n",
       "SZ1218-020512C9D-3A  24631416     246.0         1     159.0         2   \n",
       "SZ1218-020512802-9B  24631650     295.0         1     159.0         3   \n",
       "\n",
       "                     test_#_5  test_#_6  test_#_7  test_#_8  test_#_9  \\\n",
       "serial_number                                                           \n",
       "SZ1218-020512D1C-BA         1         1         1         1         1   \n",
       "SZ1218-020512C9D-3A         2         1         2         1         1   \n",
       "SZ1218-020512802-9B         3         1         3         1         1   \n",
       "\n",
       "                     test_#_10     test_#_11    test_#_12  test_#_13  \\\n",
       "serial_number                                                          \n",
       "SZ1218-020512D1C-BA     2339.0  2.018011e+13  542190876.0          1   \n",
       "SZ1218-020512C9D-3A     2339.0  2.018011e+13  542190749.0          1   \n",
       "SZ1218-020512802-9B     2339.0  2.018011e+13  542189570.0          1   \n",
       "\n",
       "                     test_#_14  test_#_15  test_#_16  test_#_17  test_#_18  \\\n",
       "serial_number                                                                \n",
       "SZ1218-020512D1C-BA   518426.0          1      246.0          1      159.0   \n",
       "SZ1218-020512C9D-3A   518426.0          1      246.0          1      159.0   \n",
       "SZ1218-020512802-9B   518426.0          1      295.0          1      159.0   \n",
       "\n",
       "                     test_#_19  test_#_20  test_#_21  test_#_22  test_#_23  \\\n",
       "serial_number                                                                \n",
       "SZ1218-020512D1C-BA          1          1          1          1          1   \n",
       "SZ1218-020512C9D-3A          2          2          1          2          1   \n",
       "SZ1218-020512802-9B          3          3          1          3          1   \n",
       "\n",
       "                     test_#_24  test_#_25     test_#_26  test_#_27  test_#_28  \\\n",
       "serial_number                                                                   \n",
       "SZ1218-020512D1C-BA          1     2339.0  2.018011e+13          1   333091.0   \n",
       "SZ1218-020512C9D-3A          1     2339.0  2.018011e+13          2   333091.0   \n",
       "SZ1218-020512802-9B          1     2339.0  2.018011e+13          1   333091.0   \n",
       "\n",
       "                     test_#_29  test_#_30  test_#_31  test_#_32  test_#_33  \\\n",
       "serial_number                                                                \n",
       "SZ1218-020512D1C-BA          1      246.0          1      159.0          1   \n",
       "SZ1218-020512C9D-3A          1      246.0          1      159.0          2   \n",
       "SZ1218-020512802-9B          1      295.0          1      159.0          3   \n",
       "\n",
       "                     test_#_34  test_#_35  test_#_36  test_#_37  test_#_38  \\\n",
       "serial_number                                                                \n",
       "SZ1218-020512D1C-BA          1          1          1          1          1   \n",
       "SZ1218-020512C9D-3A          2          1          2          1          1   \n",
       "SZ1218-020512802-9B          3          1          3          1          1   \n",
       "\n",
       "                     test_#_39     test_#_40  test_#_41  test_#_42  test_#_43  \\\n",
       "serial_number                                                                   \n",
       "SZ1218-020512D1C-BA     2339.0  2.018011e+13   333096.0          1      246.0   \n",
       "SZ1218-020512C9D-3A     2339.0  2.018011e+13   333096.0          1      246.0   \n",
       "SZ1218-020512802-9B     2339.0  2.018011e+13   333096.0          1      295.0   \n",
       "\n",
       "                     test_#_44  test_#_45  test_#_46  test_#_47  test_#_48  \\\n",
       "serial_number                                                                \n",
       "SZ1218-020512D1C-BA          1      159.0          1          1          1   \n",
       "SZ1218-020512C9D-3A          1      159.0          2          2          1   \n",
       "SZ1218-020512802-9B          1      159.0          3          3          1   \n",
       "\n",
       "                     test_#_49  ...  test_#_1514  test_#_1515  test_#_1516  \\\n",
       "serial_number                   ...                                          \n",
       "SZ1218-020512D1C-BA          1  ...            1            1       2339.0   \n",
       "SZ1218-020512C9D-3A          2  ...            1            1       2339.0   \n",
       "SZ1218-020512802-9B          3  ...            1            1       2339.0   \n",
       "\n",
       "                      test_#_1517  test_#_1518  test_#_1519  test_#_1520  \\\n",
       "serial_number                                                              \n",
       "SZ1218-020512D1C-BA  2.018011e+13        109.0     19.95112            1   \n",
       "SZ1218-020512C9D-3A  2.018011e+13        109.0     19.89527            1   \n",
       "SZ1218-020512802-9B  2.018011e+13        109.0     20.06003            1   \n",
       "\n",
       "                     test_#_1521  test_#_1522  test_#_1523  test_#_1524  \\\n",
       "serial_number                                                             \n",
       "SZ1218-020512D1C-BA     333138.0            1        246.0            1   \n",
       "SZ1218-020512C9D-3A     333138.0            1        246.0            1   \n",
       "SZ1218-020512802-9B     333138.0            1        295.0            1   \n",
       "\n",
       "                     test_#_1525  test_#_1526  test_#_1527  test_#_1528  \\\n",
       "serial_number                                                             \n",
       "SZ1218-020512D1C-BA        159.0            1            1            1   \n",
       "SZ1218-020512C9D-3A        159.0            2            2            2   \n",
       "SZ1218-020512802-9B        159.0            3            3            3   \n",
       "\n",
       "                     test_#_1529  test_#_1530  test_#_1531   test_#_1532  \\\n",
       "serial_number                                                              \n",
       "SZ1218-020512D1C-BA            1            1       2339.0  2.018011e+13   \n",
       "SZ1218-020512C9D-3A            1            1       2339.0  2.018011e+13   \n",
       "SZ1218-020512802-9B            1            1       2339.0  2.018011e+13   \n",
       "\n",
       "                     test_#_1533  test_#_1534  test_#_1535  test_#_1536  \\\n",
       "serial_number                                                             \n",
       "SZ1218-020512D1C-BA        110.0     333098.0            1        246.0   \n",
       "SZ1218-020512C9D-3A        110.0     333098.0            1        246.0   \n",
       "SZ1218-020512802-9B        110.0     333098.0            1        295.0   \n",
       "\n",
       "                     test_#_1537  test_#_1538  test_#_1539  test_#_1540  \\\n",
       "serial_number                                                             \n",
       "SZ1218-020512D1C-BA            1        159.0            1            1   \n",
       "SZ1218-020512C9D-3A            1        159.0            2            2   \n",
       "SZ1218-020512802-9B            1        159.0            3            3   \n",
       "\n",
       "                     test_#_1541  test_#_1542  test_#_1543  test_#_1544  \\\n",
       "serial_number                                                             \n",
       "SZ1218-020512D1C-BA            1            1            1       2339.0   \n",
       "SZ1218-020512C9D-3A            2            1            1       2339.0   \n",
       "SZ1218-020512802-9B            3            1            1       2339.0   \n",
       "\n",
       "                      test_#_1545  test_#_1546  test_#_1547  test_#_1548  \\\n",
       "serial_number                                                              \n",
       "SZ1218-020512D1C-BA  2.018011e+13        111.0            1     333092.0   \n",
       "SZ1218-020512C9D-3A  2.018011e+13        111.0            1     333092.0   \n",
       "SZ1218-020512802-9B  2.018011e+13        111.0            1     333092.0   \n",
       "\n",
       "                     test_#_1549  test_#_1550  test_#_1551  test_#_1552  \\\n",
       "serial_number                                                             \n",
       "SZ1218-020512D1C-BA            1          246            1          159   \n",
       "SZ1218-020512C9D-3A            1          246            1          159   \n",
       "SZ1218-020512802-9B            1          295            1          159   \n",
       "\n",
       "                     test_#_1553  test_#_1554  test_#_1555  test_#_1556  \\\n",
       "serial_number                                                             \n",
       "SZ1218-020512D1C-BA            1            1            1            1   \n",
       "SZ1218-020512C9D-3A            2            2            1            2   \n",
       "SZ1218-020512802-9B            3            3            1            3   \n",
       "\n",
       "                     test_#_1557  test_#_1558  test_#_1559   test_#_1560  \\\n",
       "serial_number                                                              \n",
       "SZ1218-020512D1C-BA            1            1       2339.0  2.018011e+13   \n",
       "SZ1218-020512C9D-3A            1            1       2339.0  2.018011e+13   \n",
       "SZ1218-020512802-9B            1            1       2339.0  2.018011e+13   \n",
       "\n",
       "                     test_#_1561  test_#_1562  Diagnosis  \n",
       "serial_number                                             \n",
       "SZ1218-020512D1C-BA     333097.0            1      False  \n",
       "SZ1218-020512C9D-3A     333097.0            1      False  \n",
       "SZ1218-020512802-9B     333097.0            1          ?  \n",
       "\n",
       "[3 rows x 1564 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = raw_data.copy()\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping 8162 samples (patients) without diagnosis taging (? / not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False        68501\n",
       "?             7718\n",
       "True           503\n",
       "not found      444\n",
       "Name: Diagnosis, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['Diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df= raw_df[raw_df['Diagnosis'].isin (['True','False'])]\n",
    "raw_df['Diagnosis'] = np.where(raw_df['Diagnosis']=='True',1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(df):\n",
    "    print(\"Number of Attributes:\")\n",
    "    print(df.shape[1]-1)\n",
    "\n",
    "    print(\"Number of samples:\")\n",
    "    print(df.shape[0])\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "    print(\"Number of samples after removing duplicates:\")\n",
    "    print(df.shape[0])\n",
    "    \n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['Diagnosis'].value_counts(normalize=True))\n",
    "    \n",
    "    #print(\"\\nNumber of NaNs:\")\n",
    "    #print(df.isnull().sum())\n",
    "    \n",
    "    null_df = pd.DataFrame({'num_nulls': df.isnull().sum()})\n",
    "    null_df = null_df[null_df['num_nulls']>0]\n",
    "    print(\"\\nNumber of features with NaNs:\",'\\n',null_df.shape[0])\n",
    "    print(\"\\nMax NaNs in a feature:\",'\\n',null_df['num_nulls'].max())\n",
    "    \n",
    "    #print(\"\\nData Types\")\n",
    "    #print(df.dtypes)\n",
    "\n",
    "    #print(\"\\nDescriptive Statistics:\")\n",
    "    #print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Attributes:\n",
      "1563\n",
      "Number of samples:\n",
      "69004\n",
      "Number of samples after removing duplicates:\n",
      "69004\n",
      "\n",
      "Class distribution:\n",
      "0    0.992711\n",
      "1    0.007289\n",
      "Name: Diagnosis, dtype: float64\n",
      "\n",
      "Number of features with NaNs: \n",
      " 723\n",
      "\n",
      "Max NaNs in a feature: \n",
      " 1998\n"
     ]
    }
   ],
   "source": [
    "explore(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "#### There are two main issues we need to address:<br>\n",
    "1) The `data is highly imbalanced`. Only 0.07!! percent of the patients are diagnosed positive to the virus.<br>\n",
    "2) There are `a LOT of features` (tests) that we need to reduce in order to:\n",
    "        - avoid overfitting (Curse of dimensionality)\n",
    "        - keep our model simple and explainable\n",
    "        - reduce computation time and storage space\n",
    "   We will use feature selection techniques and not feature extraction to save the original features for later model interpretability ability.<br>\n",
    "   \n",
    "   We will use two feature selection methods in two stages:<br>\n",
    "   \n",
    "   **Stage I**:   Filtering of uninformative features. <br>\n",
    "   **--------**<br>\n",
    "   A)  **Filter based method:** We specify some metrics and filter features based on their values. <br>\n",
    "          Our filter matrics are:\n",
    "          * Features with more then 70% NaN's\n",
    "          * Low variance features\n",
    "          * Highly correlated features\n",
    "   \n",
    "   **Stage II**: Selecting the x \"best\" features.<br> \n",
    "   **--------** <br>\n",
    "   B)  **comparative based method:** Building a comparative table to choose the x most significant features (tests). <br>\n",
    "          Our filter matrics are:\n",
    "          * Highest correlation/dependence with the target value 'Diagnosis'. \n",
    "            Measured with chi-squared for qualitative features and Pearson’s correlation absolute value for \n",
    "            quantitative\n",
    "          * Highest importance. Measured by using Algorithms that have embedded feature selection \n",
    "            methods like Regularization.       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage I: Preprossesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w = raw_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping features with too much missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_feat_with_to_many_nan(df, AT_LEAST_NON_MISSING_PERCENT = 0.7):\n",
    "    print ( \"\\nStart drop NaNs features function\")\n",
    "    #AT_LEAST_NON_MISSING_PERCENT = 0.7  # require at most 30% missing values per feature\n",
    "    AT_LEAST_NON_MISSING = math.floor(AT_LEAST_NON_MISSING_PERCENT * len(df))\n",
    "    n_cols = len(df.columns)\n",
    "    df.dropna(axis=1, thresh=AT_LEAST_NON_MISSING, inplace=True)    \n",
    "    \n",
    "    print(\"{} features dropped dut to high NaN's proportion\".format(n_cols - len(df.columns)))\n",
    "    print(\"{} features in data-set after NaN's drop.\".format(df.shape[1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_feat_with_low_var(df, CENTERED_PERCENT=0.8, LOW_VAR_VAL=0.2):\n",
    "    print ( \"\\nStart drop low var features function\")\n",
    "    features_list = df.columns.tolist()\n",
    "    features_list.remove('Diagnosis')\n",
    "    \n",
    "    low_var_features_list = []\n",
    "    categorical_features_list = []\n",
    "    numerical_features_list = []\n",
    "\n",
    "    #dividing the features into categorical and numerical\n",
    "    for ftr in features_list:\n",
    "        if df[ftr].nunique()<=20:\n",
    "            categorical_features_list.append(ftr)\n",
    "        else:\n",
    "            numerical_features_list.append(ftr)\n",
    "\n",
    "    #categorical features with low variance\n",
    "    for ftr in categorical_features_list:\n",
    "        if df[ftr].value_counts(normalize = True).max() >= CENTERED_PERCENT:\n",
    "            low_var_features_list.append(ftr)\n",
    "\n",
    "    #numerical features with low variance\n",
    "    for ftr in numerical_features_list:\n",
    "        if df[ftr].var() <= LOW_VAR_VAL:\n",
    "            low_var_features_list.append(ftr)          \n",
    "\n",
    "    df.drop(low_var_features_list,axis=1,inplace=True)\n",
    "    \n",
    "    print(\"{} features dropped due to low variance\".format(len(low_var_features_list)))\n",
    "    print(\"{} features in data-set after low var drop\".format(df.shape[1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping features that are highly correlared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_corr_feat(df, HIGH_CORR_VAL=0.9):\n",
    "    print ( \"\\nStart drop correlated features function\")\n",
    "    features_list = df.columns.tolist()\n",
    "    features_list.remove('Diagnosis')\n",
    "\n",
    "    new_ftr_list = []\n",
    "    rmv_list = []\n",
    "    corr_ftr_dict = {}\n",
    "\n",
    "    for ftr in features_list:\n",
    "        if ftr in rmv_list:\n",
    "            continue\n",
    "        else:\n",
    "            new_ftr_list.append(ftr)\n",
    "        for j in features_list:\n",
    "            if j in new_ftr_list:\n",
    "                continue\n",
    "            if j in rmv_list:\n",
    "                continue\n",
    "            if abs(df[[ftr, j]].corr().iloc[0,1]) >= HIGH_CORR_VAL:\n",
    "                rmv_list.append(j)\n",
    "                corr_ftr_dict[ftr] = j\n",
    "            \n",
    "    df.drop(rmv_list,axis=1,inplace=True)\n",
    "    \n",
    "    print(\"{} features dropped due to high correlation\".format(len(rmv_list)))\n",
    "    print(\"{} features in data-set after correlation drop\".format(df.shape[1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing features with NaN values\n",
    "qualitative (categorical) features with the mode, and quanitative with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_impu(df):\n",
    "    print ( \"\\nStart features imputation function\")\n",
    "    features_list = df.columns.tolist()\n",
    "    missing_list = df.isnull().sum()[df.isnull().sum()>0].keys().tolist()\n",
    "\n",
    "    qualitative_ftr_miss_list = []\n",
    "    quanitative_ftr_miss_list = []\n",
    "\n",
    "    imputer_qual = SimpleImputer(strategy='most_frequent')\n",
    "    imputer_quan = SimpleImputer()\n",
    "    \n",
    "    if len(missing_list) > 0:\n",
    "        for ftr in missing_list:\n",
    "            if df[ftr].nunique() <= 20:\n",
    "                qualitative_ftr_miss_list.append(ftr)\n",
    "            else:\n",
    "                quanitative_ftr_miss_list.append(ftr)\n",
    "\n",
    "    if len(qualitative_ftr_miss_list) > 0:\n",
    "        df[qualitative_ftr_miss_list] = imputer_qual.fit_transform(df.loc[:, qualitative_ftr_miss_list])\n",
    "    if len(quanitative_ftr_miss_list) > 0:\n",
    "        df[quanitative_ftr_miss_list] = imputer_quan.fit_transform(df.loc[:, quanitative_ftr_miss_list])\n",
    "    \n",
    "    print( \"Number of features with NaNs:\", len(df.isnull().sum()[df.isnull().sum() > 0]) )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage I preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = drop_feat_with_to_many_nan(df)\n",
    "    df = drop_feat_with_low_var(df)\n",
    "    df = drop_high_corr_feat(df)\n",
    "    df = feat_impu(df)\n",
    "        \n",
    "    print ( \"\\nEnd preprocessing\")\n",
    "    print(\"\\nNumber of Attributes:\", df.shape[1]-1)\n",
    "    print(\"Number of samples:\", df.shape[0])\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['Diagnosis'].value_counts(normalize=True))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage II: Ensemble feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic creating less imbalance data-set\n",
    "To address the imbalance issue, we will create a less imbalance data-set with 1:3 target class relation for further use of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance(df, SCALE = 3):\n",
    "    positive_df = df[df['Diagnosis'] == 1]\n",
    "    negative_df = df[df['Diagnosis'] == 0]\n",
    "    n = int(SCALE * positive_df.shape[0])\n",
    "    rebalance_df = pd.concat([positive_df, negative_df.sample(n)])\n",
    "\n",
    "    print(\"\\nrebalanced data class distribution:\")\n",
    "    print(rebalance_df['Diagnosis'].value_counts(normalize=True))\n",
    "    print(\"rebalanced Number of samples:\", rebalance_df.shape[0])\n",
    "    return rebalance_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(df, num_feats = 20):\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "    cor_list = []\n",
    "    feature_name = X.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    print(\"\\n\", str(len(cor_feature)), 'selected features by pearson correlation')\n",
    "    return cor_support, cor_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_selector(df, num_feats = 20):\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "    feature_list = X.columns.tolist()\n",
    "    qualitative_ftr_list = []\n",
    "    \n",
    "    for ftr in feature_list:\n",
    "        if df[ftr].nunique() <= 20:\n",
    "            qualitative_ftr_list.append(ftr)\n",
    "    \n",
    "    #X = df.loc[:, qualitative_ftr_list]\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    chi_selector = SelectKBest(chi2, k=num_feats)\n",
    "    chi_selector.fit(X_norm, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "    print(\"\\n\", str(len(chi_feature)), 'selected features by chi-squared')\n",
    "    return chi_support, chi_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression with L1 norm regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_selector(df, num_feats = 20):\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    X_norm = StandardScaler().fit_transform(X)\n",
    "\n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\"), max_features=num_feats)\n",
    "    embeded_lr_selector.fit(X_norm, y)\n",
    "\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "    print(\"\\n\", str(len(embeded_lr_feature)), 'selected features by lasso')\n",
    "    return embeded_lr_support, embeded_lr_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest_selector(df, num_feats = 20):\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    X_norm = StandardScaler().fit_transform(X)\n",
    "\n",
    "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=50), max_features=num_feats)\n",
    "    embeded_rf_selector.fit(X, y)\n",
    "\n",
    "    embeded_rf_support = embeded_rf_selector.get_support()\n",
    "    embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "    print(\"\\n\", str(len(embeded_rf_feature)), 'selected features by random forest')\n",
    "    return embeded_rf_support, embeded_rf_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection summarize table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ftr_comp_table(df, num_select_feats = 20, rblnc_scale = 1):\n",
    "    df = preprocess(df)\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    feature_name = X.columns.tolist()\n",
    "    \n",
    "    reblnc_df = rebalance(df, SCALE = rblnc_scale)\n",
    "\n",
    "    cor_support, cor_feature = cor_selector(df, num_feats = num_select_feats)\n",
    "    chi_support, chi_feature = chi_selector(df, num_feats = num_select_feats)\n",
    "    embeded_lr_support, embeded_lr_feature = lasso_selector(reblnc_df, num_feats = num_select_feats)\n",
    "    embeded_rf_support, embeded_rf_feature = randomforest_selector(reblnc_df, num_feats = num_select_feats)\n",
    "        \n",
    "    # put all selection together\n",
    "    feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support,\\\n",
    "                                         'Lasso':embeded_lr_support, 'Random Forest':embeded_rf_support})\n",
    "    # count the selected times for each feature\n",
    "    feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "    # display by the top features\n",
    "    feature_selection_tbl = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "    feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "    return feature_selection_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start drop NaNs features function\n",
      "0 features dropped dut to high NaN's proportion\n",
      "1564 features in data-set after NaN's drop.\n",
      "\n",
      "Start drop low var features function\n",
      "268 features dropped due to low variance\n",
      "1296 features in data-set after low var drop\n",
      "\n",
      "Start drop correlated features function\n",
      "1255 features dropped due to high correlation\n",
      "41 features in data-set after correlation drop\n",
      "\n",
      "Start features imputation function\n",
      "Number of features with NaNs: 0\n",
      "\n",
      "End preprocessing\n",
      "\n",
      "Number of Attributes: 40\n",
      "Number of samples: 69004\n",
      "\n",
      "Class distribution:\n",
      "0    0.992711\n",
      "1    0.007289\n",
      "Name: Diagnosis, dtype: float64\n",
      "\n",
      "rebalanced data class distribution:\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: Diagnosis, dtype: float64\n",
      "rebalanced Number of samples: 4527\n",
      "\n",
      " 8 selected features by pearson correlation\n",
      "\n",
      " 8 selected features by chi-squared\n",
      "\n",
      " 8 selected features by lasso\n",
      "\n",
      " 8 selected features by random forest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Chi-2</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>test_#_950</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_#_2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>test_#_879</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>test_#_728</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test_#_538</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature  Pearson  Chi-2  Lasso  Random Forest  Total\n",
       "33  test_#_950     True   True   True          False      3\n",
       "2     test_#_2     True   True   True          False      3\n",
       "29  test_#_879     True   True  False          False      2\n",
       "23  test_#_728    False   True  False           True      2\n",
       "17  test_#_538    False  False   True           True      2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = ftr_comp_table(df_w, num_select_feats = 8, rblnc_scale = 8)\n",
    "tbl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal Function\n",
    "### The pipeline of the entire process\n",
    "The function gets as `INPUT` --> Full raw data (after minor specific corrections, if needed). <br>\n",
    "Function `OUTPUT` --> Processed data with only X features, X can be dynamically defined by the user. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_model(df, num_select_feat = 20, rbalc_scale = 1):\n",
    "    \n",
    "    preprocess(df)\n",
    "    ftr_select_tbl = ftr_comp_table(df, num_select_feats = num_select_feat, rblnc_scale = rbalc_scale)   \n",
    "    \n",
    "    selected_ftr_list = ftr_select_tbl.iloc[0:num_select_feat, 0].tolist()\n",
    "    selected_ftr_list.append('Diagnosis')\n",
    "    model_df_output = df.loc[:, selected_ftr_list]\n",
    "    return model_df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start drop NaNs features function\n",
      "0 features dropped dut to high NaN's proportion\n",
      "41 features in data-set after NaN's drop.\n",
      "\n",
      "Start drop low var features function\n",
      "0 features dropped due to low variance\n",
      "41 features in data-set after low var drop\n",
      "\n",
      "Start drop correlated features function\n",
      "0 features dropped due to high correlation\n",
      "41 features in data-set after correlation drop\n",
      "\n",
      "Start features imputation function\n",
      "Number of features with NaNs: 0\n",
      "\n",
      "End preprocessing\n",
      "\n",
      "Number of Attributes: 40\n",
      "Number of samples: 69004\n",
      "\n",
      "Class distribution:\n",
      "0    0.992711\n",
      "1    0.007289\n",
      "Name: Diagnosis, dtype: float64\n",
      "\n",
      "Start drop NaNs features function\n",
      "0 features dropped dut to high NaN's proportion\n",
      "41 features in data-set after NaN's drop.\n",
      "\n",
      "Start drop low var features function\n",
      "0 features dropped due to low variance\n",
      "41 features in data-set after low var drop\n",
      "\n",
      "Start drop correlated features function\n",
      "0 features dropped due to high correlation\n",
      "41 features in data-set after correlation drop\n",
      "\n",
      "Start features imputation function\n",
      "Number of features with NaNs: 0\n",
      "\n",
      "End preprocessing\n",
      "\n",
      "Number of Attributes: 40\n",
      "Number of samples: 69004\n",
      "\n",
      "Class distribution:\n",
      "0    0.992711\n",
      "1    0.007289\n",
      "Name: Diagnosis, dtype: float64\n",
      "\n",
      "rebalanced data class distribution:\n",
      "0    0.75\n",
      "1    0.25\n",
      "Name: Diagnosis, dtype: float64\n",
      "rebalanced Number of samples: 2012\n",
      "\n",
      " 15 selected features by pearson correlation\n",
      "\n",
      " 15 selected features by chi-squared\n",
      "\n",
      " 15 selected features by lasso\n",
      "\n",
      " 13 selected features by random forest\n"
     ]
    }
   ],
   "source": [
    "model_df = data_for_model(df_w, num_select_feat = 15, rbalc_scale = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69004, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ;-) Bonus  - Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting a Random Forest predictor over our processed data, we got pretty good scores over the training set (about 97% accuracy and f1-score), but much less performance over the test set (only about 68% accuracy and f1-score).\n",
    "For this kind of problem (predicting virus illness), we don't want to miss any ill patients, so it is highly essential to have minimum type-1 error (false negative), i.e., **`high recall`**. It is also important to have low type-2 error (false positive), i.e., **`high precision`**, to avoid misdiagnosis.The **`f1-score`**, the harmonic mean of precision and recall, is a suitable measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_#_760</th>\n",
       "      <th>test_#_0</th>\n",
       "      <th>test_#_932</th>\n",
       "      <th>test_#_744</th>\n",
       "      <th>test_#_728</th>\n",
       "      <th>test_#_2</th>\n",
       "      <th>test_#_950</th>\n",
       "      <th>test_#_914</th>\n",
       "      <th>test_#_896</th>\n",
       "      <th>test_#_879</th>\n",
       "      <th>test_#_862</th>\n",
       "      <th>test_#_777</th>\n",
       "      <th>test_#_774</th>\n",
       "      <th>test_#_649</th>\n",
       "      <th>test_#_538</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serial_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SZ1218-020512D1C-BA</th>\n",
       "      <td>1</td>\n",
       "      <td>24631336</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ1218-020512C9D-3A</th>\n",
       "      <td>1</td>\n",
       "      <td>24631416</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ1218-02051280A-A3</th>\n",
       "      <td>1</td>\n",
       "      <td>24631652</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     test_#_760  test_#_0  test_#_932  test_#_744  test_#_728  \\\n",
       "serial_number                                                                   \n",
       "SZ1218-020512D1C-BA           1  24631336           1           1           1   \n",
       "SZ1218-020512C9D-3A           1  24631416           1           2           2   \n",
       "SZ1218-02051280A-A3           1  24631652           1           1           4   \n",
       "\n",
       "                     test_#_2  test_#_950  test_#_914  test_#_896  test_#_879  \\\n",
       "serial_number                                                                   \n",
       "SZ1218-020512D1C-BA         1        48.0           1           1           1   \n",
       "SZ1218-020512C9D-3A         1        44.0           1           1           1   \n",
       "SZ1218-02051280A-A3         1        48.0           1           1           1   \n",
       "\n",
       "                     test_#_862  test_#_777  test_#_774  test_#_649  \\\n",
       "serial_number                                                         \n",
       "SZ1218-020512D1C-BA           1        48.0        63.0        12.0   \n",
       "SZ1218-020512C9D-3A           2        44.0        63.0        12.0   \n",
       "SZ1218-02051280A-A3           1        48.0        63.0         8.0   \n",
       "\n",
       "                     test_#_538  Diagnosis  \n",
       "serial_number                               \n",
       "SZ1218-020512D1C-BA        53.0          0  \n",
       "SZ1218-020512C9D-3A        13.0          0  \n",
       "SZ1218-02051280A-A3        46.0          0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rebalanced data class distribution:\n",
      "0    0.75\n",
      "1    0.25\n",
      "Name: Diagnosis, dtype: float64\n",
      "rebalanced Number of samples: 2012\n"
     ]
    }
   ],
   "source": [
    "model_df_1 = rebalance(model_df, SCALE = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df_1.drop('Diagnosis', axis=1)\n",
    "y = model_df_1['Diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(random_state=1)\n",
    "\n",
    "classifiers = [('RF', clf1)]\n",
    "fi_classifiers_list = ['RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    import itertools\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title,fontsize=14)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=14)\n",
    "    plt.xlabel('Predicted label',fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================================================\n",
      "RF classifier:\n",
      "         \tTrain Accuracy: 0.98\n",
      "         \tTest Accuracy: 0.71\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      1207\n",
      "           1       1.00      0.91      0.95       402\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1609\n",
      "   macro avg       0.99      0.96      0.97      1609\n",
      "weighted avg       0.98      0.98      0.98      1609\n",
      "\n",
      "\n",
      "Train Confusion Matrix:\n",
      "      0    1\n",
      "0  1207    0\n",
      "1    35  367\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82       302\n",
      "           1       0.35      0.18      0.24       101\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       403\n",
      "   macro avg       0.56      0.53      0.53       403\n",
      "weighted avg       0.66      0.71      0.68       403\n",
      "\n",
      "\n",
      "Test Confusion Matrix:\n",
      "     0   1\n",
      "0  269  33\n",
      "1   83  18\n",
      "\n",
      "Feature Importance by PermutationImportance method:\n",
      "Explained as: feature importances\n",
      "\n",
      "Feature importances, computed as a decrease in score when feature\n",
      "values are permuted (i.e. become noise). This is also known as \n",
      "permutation importance.\n",
      "\n",
      "If feature importances are computed on the same data as used for training, \n",
      "they don't reflect importance of features for generalization. Use a held-out\n",
      "dataset if you want generalization feature importances.\n",
      "\n",
      "0.1392 ± 0.0081  test_#_744\n",
      "0.1320 ± 0.0187  test_#_0\n",
      "0.1111 ± 0.0096  test_#_728\n",
      "0.1096 ± 0.0083  test_#_538\n",
      "0.1038 ± 0.0072  test_#_862\n",
      "0.0648 ± 0.0066  test_#_649\n",
      "0.0459 ± 0.0028  test_#_879\n",
      "0.0409 ± 0.0064  test_#_777\n",
      "        … 7 more …         \n",
      "\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print (\"\\n===========================================================================================================\")\n",
    "results_train = y_train.to_frame()\n",
    "results_test = y_test.to_frame()\n",
    "\n",
    "#fi_df = {}\n",
    "fi_df = pd.DataFrame(X_train.columns, columns = ['feature'])\n",
    "confusion_matrixs_train = {}\n",
    "confusion_matrixs_test = {}\n",
    "\n",
    "for clf_name, clf in classifiers:\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    results_train[clf_name] = clf.predict(X_train)\n",
    "    results_test[clf_name] = clf.predict(X_test)\n",
    "    \n",
    "    print ('{:2} classifier:\\n \\\n",
    "        \\tTrain Accuracy: {:.2f}\\n \\\n",
    "        \\tTest Accuracy: {:.2f}'\\\n",
    "               .format(clf_name,\n",
    "               clf.score(X_train, y_train),\n",
    "               clf.score(X_test, y_test)))\n",
    "\n",
    "    # Compute train classification report\n",
    "    print (\"\\nTrain Classification Report:\")\n",
    "    print(classification_report(y_true=y_train,\n",
    "                            y_pred=y_pred_train))\n",
    "    \n",
    "    # Compute train confusion matrix\n",
    "    cnf_matrix_train = confusion_matrix(y_true=y_train, y_pred=results_train[clf_name])\n",
    "    confusion_matrixs_train[clf_name] = cnf_matrix_train\n",
    "    \n",
    "    print (\"\\nTrain Confusion Matrix:\")\n",
    "    print (pd.DataFrame(cnf_matrix_train,\n",
    "             index = clf.classes_,\n",
    "             columns = clf.classes_))\n",
    "    \n",
    "    # Compute test classification report    \n",
    "    print (\"\\nTest Classification Report:\")\n",
    "    print(classification_report(y_true=y_test,\n",
    "                            y_pred=y_pred_test))\n",
    "\n",
    "    # Compute test confusion matrix\n",
    "    cnf_matrix_test = confusion_matrix(y_true=y_test, y_pred=results_test[clf_name])\n",
    "    confusion_matrixs_test[clf_name] = cnf_matrix_test\n",
    "\n",
    "    print (\"\\nTest Confusion Matrix:\")\n",
    "    print (pd.DataFrame(cnf_matrix_test,\n",
    "             index = clf.classes_,\n",
    "             columns = clf.classes_))\n",
    "    \n",
    "    # Compute feature importance matrix\n",
    "    print (\"\\nFeature Importance by PermutationImportance method:\")\n",
    "    perm = PermutationImportance(clf).fit(X_train, y_train)\n",
    "    print ( eli5.format_as_text(eli5.explain_weights(perm, top=8, feature_names = X_train.columns.tolist())) )\n",
    "\n",
    "    if clf_name in fi_classifiers_list:\n",
    "        fi_df[clf_name+'_fi_score'] = clf.feature_importances_[:, ]\n",
    "        fi_df[clf_name+'_rel_fi_score'] = fi_df[clf_name+'_fi_score'] / fi_df[clf_name+'_fi_score'].max()\n",
    "    \n",
    "    \n",
    "    print (\"\\n=================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.figure(figsize=(8,5))\n",
    "plot_confusion_matrix(confusion_matrixs_train['RF'],\n",
    "                      classes=clf.classes_,normalize= False,  title='Train Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.figure(figsize=(8,5))\n",
    "plot_confusion_matrix(confusion_matrixs_test['RF'],\n",
    "                      classes=clf.classes_,normalize= False,  title='Test Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> THE END <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
